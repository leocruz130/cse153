{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82bb727c",
   "metadata": {
    "id": "82bb727c"
   },
   "source": [
    "# Homework 2  \n",
    "\n",
    "The goal of this assignment is experiment with classification pipelines (in this case, for instrument classification) using spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e9780f",
   "metadata": {
    "id": "22e9780f"
   },
   "outputs": [],
   "source": [
    "# Set this yourself depending where you put the files\n",
    "dataroot = \"/Users/lcruzpaz/Documents/Git_Repositories/cse153/homework2\"\n",
    "# On the autograder it should be here:\n",
    "# dataroot = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58eb3564",
   "metadata": {
    "id": "58eb3564"
   },
   "outputs": [],
   "source": [
    "# !pip install librosa\n",
    "# !pip install torch\n",
    "# !pip install glob\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b490c05",
   "metadata": {
    "id": "0b490c05"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884745cd",
   "metadata": {
    "id": "884745cd"
   },
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True) # Try to make things less random, though not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a5f028",
   "metadata": {
    "id": "55a5f028"
   },
   "outputs": [],
   "source": [
    "audio_paths = glob.glob(dataroot + \"/nsynth_subset/*.wav\")\n",
    "random.seed(0)\n",
    "random.shuffle(audio_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f5ab0b",
   "metadata": {
    "id": "57f5ab0b"
   },
   "outputs": [],
   "source": [
    "if not len(audio_paths):\n",
    "    print(\"You probably need to set the dataroot folder correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d7a98a0",
   "metadata": {
    "id": "8d7a98a0"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 8000 # Very low sample rate, just so things run quickly\n",
    "N_MFCC = 13\n",
    "INSTRUMENT_MAP = {'guitar': 0, 'vocal': 1} # Only two classes (also so that things run quickly)\n",
    "NUM_CLASSES = len(INSTRUMENT_MAP)\n",
    "\n",
    "# If we used all the classes we would have:\n",
    "# INSTRUMENT_MAP = {\n",
    "#     'bass': 0, 'brass': 1, 'flute': 2, 'guitar': 3,\n",
    "#     'keyboard': 4, 'mallet': 5, 'organ': 6, 'reed': 7,\n",
    "#     'string': 8, 'synth_lead': 9, 'vocal': 10\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de9554",
   "metadata": {
    "id": "e9de9554"
   },
   "source": [
    "1. Extract prediction labels and construct waveforms\n",
    "\n",
    "`extract_waveform()`\n",
    "\n",
    "**Inputs**\n",
    "- `path`: A string that represents a path to the wav file\n",
    "\n",
    "**Outputs**\n",
    "- `waveform`: an array containing the waveform; use librosa.load, remember to set the sample rate correctly\n",
    "\n",
    "`extract_label()`\n",
    "\n",
    "**Inputs**\n",
    "- `path'\n",
    "\n",
    "**Outputs**\n",
    "- `label`: A integer that represents the label of the path (hint: look at the filename and make use of `INSTRUMENT_MAP`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f9838f9",
   "metadata": {
    "id": "8f9838f9"
   },
   "outputs": [],
   "source": [
    "def extract_waveform(path):\n",
    "    waveform, _ = librosa.load(path, sr =SAMPLE_RATE)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f5927c",
   "metadata": {
    "id": "c7f5927c"
   },
   "outputs": [],
   "source": [
    "def extract_label(path):\n",
    "    filename = path.split('/')[-1]\n",
    "    instrument = filename.split('_')[0]\n",
    "    label = INSTRUMENT_MAP[instrument]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19795b2d",
   "metadata": {
    "id": "19795b2d"
   },
   "outputs": [],
   "source": [
    "waveforms = [extract_waveform(p) for p in audio_paths]\n",
    "labels = [extract_label(p) for p in audio_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab24201",
   "metadata": {
    "id": "dab24201"
   },
   "source": [
    "A few simple classifiers are provided. You don't need to modify these (though the autograder will *probably* work if you'd like to experiment with architectural changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e261212",
   "metadata": {
    "id": "8e261212"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(2 * N_MFCC, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, NUM_CLASSES)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6fb390",
   "metadata": {
    "id": "9b6fb390"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc = nn.Linear(64, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pool1(nnF.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(nnF.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(nnF.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23cde9",
   "metadata": {
    "id": "0f23cde9"
   },
   "source": [
    "2. Extract mfcc features\n",
    "\n",
    "`extract_mfcc()`\n",
    "\n",
    "**Inputs**\n",
    "- `waveform`: an array containing the waveform\n",
    "\n",
    "**Outputs**\n",
    "- `feature`: a PyTorch float tensor that represents a concatenation of 13 mean values and 13 standard deviation values\n",
    "\n",
    "**Process**\n",
    "- Extract feature using `librosa.feature.mfcc`; remember to set the sample rate and n_mfcc\n",
    "- Compute 13 mean and 13 standard deviation values\n",
    "- Concatenate them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e72a796",
   "metadata": {
    "id": "0e72a796"
   },
   "outputs": [],
   "source": [
    "def extract_mfcc(w):\n",
    "    # Your code here:\n",
    "    # load using librosa.feature.mfcc\n",
    "    # extract mean and std\n",
    "    # concatenate\n",
    "    mfcc_features = librosa.feature.mfcc(\n",
    "        y = w,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_mfcc=N_MFCC\n",
    "    )\n",
    "    mfcc_mean = np.mean(mfcc_features, axis=1)\n",
    "    mfcc_std = np.std(mfcc_features, axis=1)\n",
    "\n",
    "    features = np.concatenate([mfcc_mean, mfcc_std])\n",
    "\n",
    "    return torch.FloatTensor(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83dbca5",
   "metadata": {
    "id": "a83dbca5"
   },
   "source": [
    "## Note:\n",
    "\n",
    "The autograder will test that your MFCC features are correct, and it will *also* use them within an ML pipeline. The test_suite can be used to run the full pipeline after you've implemented these functions. If you've implemented your features correctly this should \"just work\" and you'll be able to upload the trained; this is mostly here just so that you can see how the full pipeline works (which will be useful when you develop your own pipelines for Assignment 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4577a",
   "metadata": {
    "id": "52f4577a"
   },
   "source": [
    "3. Extract spectrograms\n",
    "\n",
    "`extract_spec()`\n",
    "\n",
    "**Inputs**\n",
    "- `waveform`: an array containing the waveform\n",
    "\n",
    "**Outputs**\n",
    "- `feature`: a PyTorch float tensor that contains a spectrogram\n",
    "\n",
    "**Process**\n",
    "- apply STFT to the given waveform\n",
    "- square the absolute values of the complex numbers from the STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3325a7e",
   "metadata": {
    "id": "a3325a7e"
   },
   "outputs": [],
   "source": [
    "def extract_spec(w):\n",
    "    # Your code here\n",
    "    # load\n",
    "    # take squared absolute values\n",
    "    n_fft = 512 # STFT parameter; higher n_fft = higher frequency resolution\n",
    "    hop_length = n_fft // 4 # STFT parameter; smaller hop_length = higher time resolution\n",
    "    stft_complex = librosa.stft(y=w, n_fft=n_fft, hop_length=hop_length)\n",
    "    spectogram = np.abs(stft_complex)**2\n",
    "    spect_tensor = torch.tensor(spectogram,dtype=torch.float32)\n",
    "    return spect_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d252d",
   "metadata": {
    "id": "5e4d252d"
   },
   "source": [
    "4. Extract mel-spectrograms\n",
    "\n",
    "`extract_mel()`\n",
    "\n",
    "**Inputs**\n",
    "- `waveform`: an array containing the waveform\n",
    "- `n_mels`: number of mel bands\n",
    "- `hop_length`: hop length\n",
    "\n",
    "**Outputs**\n",
    "- `feature`: A PyTorch Float Tensor that contains a mel-spectrogram\n",
    "\n",
    "**Process**\n",
    "- generate melspectrograms with `librosa.feature.melspectrogram`; make sure to se the sample rate, n_mels, and hop_length\n",
    "- convert them to decibel units with `librosa.power_to_db`\n",
    "- normalize values to be in the range 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e994623d",
   "metadata": {
    "id": "e994623d"
   },
   "outputs": [],
   "source": [
    "def extract_mel(w, n_mels = 128, hop_length = 512):\n",
    "    # Your code here\n",
    "    # load\n",
    "    # convert to db\n",
    "    # normalize\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y = w,\n",
    "        sr= SAMPLE_RATE,\n",
    "        n_mels = n_mels,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    normalized = (mel_spec_db - mel_spec_db.min())/(mel_spec_db.max() - mel_spec_db.min())\n",
    "\n",
    "    #converting to the pytorch tensor \n",
    "    return torch.FloatTensor(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12708c02",
   "metadata": {
    "id": "12708c02"
   },
   "source": [
    "5. Extract constant-Q transform\n",
    "\n",
    "`extract_q()`\n",
    "\n",
    "**Inputs**\n",
    "- `waveform`: an array containing the waveform\n",
    "\n",
    "**Outputs**\n",
    "- `feature`: A PyTorch Float Tensor that contains a constant-Q transform\n",
    "\n",
    "**Process**\n",
    "- generate constant-Q transform with `librosa.cqt`; this one will need a higher sample rate (use 16000) to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd405c1",
   "metadata": {
    "id": "4cd405c1"
   },
   "outputs": [],
   "source": [
    "def extract_q(w):\n",
    "    n_mels = 128\n",
    "    hop_length = 512\n",
    "    cqt = librosa.cqt(\n",
    "        y=w,\n",
    "        sr=16000,\n",
    "    )\n",
    "    cqt_tensor = torch.tensor(np.abs(cqt), dtype=torch.float32)\n",
    "\n",
    "    #converting to the pytorch tenosr\n",
    "    return cqt_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f6ac5",
   "metadata": {
    "id": "4f0f6ac5"
   },
   "source": [
    "6. Pitch shift\n",
    "\n",
    "`pitch_shift()`\n",
    "\n",
    "**Inputs**\n",
    "- `waveform`: an array containing the waveform\n",
    "- `n`: number of semitones to shift by (integer, can be positive or negative)\n",
    "\n",
    "**Outputs**\n",
    "- `waveform`: a pitch-shifted waveform\n",
    "\n",
    "**Process**\n",
    "- use `librosa.effects.pitch_shift`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b71abf8f",
   "metadata": {
    "id": "b71abf8f"
   },
   "outputs": [],
   "source": [
    "def pitch_shift(w, n):\n",
    "    y_shift = librosa.effects.pitch_shift(\n",
    "        y=w,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_steps= n,\n",
    "    )\n",
    "    return y_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f16e3f0a",
   "metadata": {
    "id": "f16e3f0a"
   },
   "outputs": [],
   "source": [
    "# Code below augments the datasets\n",
    "\n",
    "augmented_waveforms = []\n",
    "augmented_labels = []\n",
    "\n",
    "for w,y in zip(waveforms,labels):\n",
    "    augmented_waveforms.append(w)\n",
    "    augmented_waveforms.append(pitch_shift(w,1))\n",
    "    augmented_waveforms.append(pitch_shift(w,-1))\n",
    "    augmented_labels += [y,y,y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c48ae",
   "metadata": {
    "id": "bc0c48ae"
   },
   "source": [
    "7. Extend the model to work for four classes.\n",
    "\n",
    "By  data augmentations, or modifying the model architecture, build a model with test accuracy > 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f9b2fc1",
   "metadata": {
    "id": "9f9b2fc1"
   },
   "outputs": [],
   "source": [
    "INSTRUMENT_MAP_7 = {'guitar_acoustic': 0, 'guitar_electronic': 1, 'vocal_acoustic': 2, 'vocal_synthetic': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5be77a1f",
   "metadata": {
    "id": "5be77a1f"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES_7 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3b2e8a9",
   "metadata": {
    "id": "b3b2e8a9"
   },
   "outputs": [],
   "source": [
    "def extract_label_7(path):\n",
    "    filename = path.split('/')[-1]\n",
    "    inst_type = '_'.join(filename.split('_')[:2])\n",
    "    return INSTRUMENT_MAP_7[inst_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d6f20d",
   "metadata": {
    "id": "42d6f20d"
   },
   "outputs": [],
   "source": [
    "def extract_mfcc_enhanced(w, sr=SAMPLE_RATE):\n",
    "    mfcc_features = librosa.feature.mfcc(\n",
    "        y=w,\n",
    "        sr=sr,\n",
    "        n_mfcc=N_MFCC,\n",
    "        hop_length=512,\n",
    "        n_fft=2048\n",
    "    )\n",
    "    \n",
    "    # Adding delta and delta-delta features to the dynamics\n",
    "    mfcc_delta = librosa.feature.delta(mfcc_features)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc_features, order=2)\n",
    "    \n",
    "    # Extract basic statistics\n",
    "    mfcc_mean = np.mean(mfcc_features, axis=1)\n",
    "    mfcc_std = np.std(mfcc_features, axis=1)\n",
    "    mfcc_min = np.min(mfcc_features, axis=1)\n",
    "    mfcc_max = np.max(mfcc_features, axis=1)\n",
    "    \n",
    "    # Delta statistics\n",
    "    delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "    delta_std = np.std(mfcc_delta, axis=1)\n",
    "    \n",
    "    # Delta2 statistics\n",
    "    delta2_mean = np.mean(mfcc_delta2, axis=1)\n",
    "    delta2_std = np.std(mfcc_delta2, axis=1)\n",
    "    \n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=w, sr=sr))\n",
    "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=w, sr=sr))\n",
    "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=w, sr=sr))\n",
    "    \n",
    "\n",
    "    \n",
    "    # Concatenate all features\n",
    "    features = np.concatenate([\n",
    "        mfcc_mean, mfcc_std, mfcc_min, mfcc_max,\n",
    "        delta_mean, delta_std,\n",
    "        delta2_mean, delta2_std,\n",
    "        [spectral_centroid, spectral_bandwidth, spectral_rolloff]\n",
    "    ])\n",
    "    \n",
    "    return torch.FloatTensor(features)\n",
    "feature_func_7 = extract_mfcc_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a05958c",
   "metadata": {
    "id": "6a05958c"
   },
   "outputs": [],
   "source": [
    "labels_7 = [extract_label_7(p) for p in audio_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d710b28f",
   "metadata": {
    "id": "d710b28f"
   },
   "outputs": [],
   "source": [
    "class ModifiedMLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size = 2*N_MFCC):\n",
    "        super(ModifiedMLPClassifier, self).__init__()\n",
    "        # Calculate input size based on our feature extraction\n",
    "        self.fc1 = nn.Linear(input_size, 256) \n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, NUM_CLASSES_7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nnF.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nnF.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nnF.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model_7 = ModifiedMLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340c848",
   "metadata": {},
   "source": [
    "DeepSeek used to debug and improve my enhanced version of my feature function and modified version of MLP Model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
